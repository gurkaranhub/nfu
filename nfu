#!/usr/bin/env perl
# nfu: Command-line numeric fu | Spencer Tipping
# Licensed under the terms of the MIT source code license

use v5.14;
use strict;
use warnings;

use Time::HiRes qw/time/;
use POSIX       qw/dup2 mkfifo setsid :sys_wait_h/;
use File::Temp  qw/tmpnam/;

use constant VERBOSE_INTERVAL => 30;

++$|;

# Setup child capture. All we need to do is wait for child pids; there's no
# formal teardown.
$SIG{CHLD} = sub {
  local ($!, $?);
  waitpid -1, WNOHANG;
};

# NB: This import is not used in nfu directly; it's here so you can use these
# functions inside aggregators.
use List::Util qw(first max maxstr min minstr reduce shuffle sum);

# Same for this, which is especially useful from aggregators because multiple
# values create multiple output rows, not multiple columns on the same output
# row.
sub row {join "\t", map s/\n//gr, @_}

# Order-preserving unique values for strings. This is just too useful not to
# provide.
sub uniq {
  my %seen;
  my @order;
  $seen{$_}++ or push @order, $_ for @_;
  @order;
}

sub frequencies {
  my %freqs;
  ++$freqs{$_} for @_;
  %freqs;
}

sub mean {scalar @_ && sum(@_) / @_}

# JSON support (if available)
our $json;
if (eval {require JSON}) {
  JSON->import;
  no warnings qw(uninitialized);
  $json = JSON->new->allow_nonref->utf8(1);
} elsif (eval {require JSON::PP}) {
  JSON::PP->import;
  no warnings qw(uninitialized);
  $json = JSON::PP->new->allow_nonref->utf8(1);
} else {
  print STDERR "note: no JSON support detected (try 'cpan install JSON')\n";
  print STDERR "nfu will soon have its own JSON parser rather than using ";
  print STDERR "a native library for this. Sorry for the inconvenience.";
}

# These are callable from evaled code
sub expand_filename_shorthands;
sub read_file {
  open my $fh, expand_filename_shorthands $_[0], 1;
  my $result = join '', <$fh>;
  close $fh;
  $result;
}

sub read_lines {
  open my $fh, expand_filename_shorthands $_[0], 1;
  my @result;
  chomp, push @result, $_ for <$fh>;
  close $fh;
  @result;
}

sub json_encode {$json->encode(@_)}
sub json_decode {$json->decode(@_)}

sub je {json_encode @_}
sub jd {json_decode @_}

# File functions
sub hadoop_ls;
sub hadoop_matched_partfile;
sub expand_filename_shorthands {
  # NB: we prepend a shell comment containing the original $f so anyone
  # downstream can get it back. This is currently used by Hadoop to reuse data
  # stored on HDFS if you write it on the command-line. (Otherwise nfu would
  # hadoop fs -text to download it, then re-upload to a tempfile.)

  my ($f, $always_make_a_command) = @_;
  my $result;
  my $original = "#$f\n";

  if (-e $f) {
    # It's really a filename, so push it onto @ARGV. If it's compressed, run it
    # through the appropriate decompressor first.
    my $piped = $f =~ s/^(.*\.gz)/cat '$1' | gzip -d/ri
                   =~ s/^(.*\.bz2)/cat '$1' | bzip -d/ri
                   =~ s/^(.*\.xz)/cat '$1' | xz -d/ri
                   =~ s/^(.*\.lzo)/cat '$1' | lzop -d/ri;
    $result = $piped =~ /\|/ ? "$original$piped |" : $piped;
  } elsif ($f =~ s/^(?:http(s?):)?\/\//"http" . ($1 || '') . ":\/\/"/e) {
    # Assume a URL and curl it
    $f = shell_quote $f;
    $result = "${original}curl $f |";
  } elsif ($f =~ s/^sh://) {
    # Execute a command and capture stdout
    $result = "$original$f |";
  } elsif ($f =~ s/^hdfs://) {
    # Use Hadoop commands to read stuff
    my $fs = join ' ', map shell_quote($_), hadoop_ls $f;
    $result = "${original}hadoop fs -text $fs |";
  } elsif ($f =~ s/^hdfsjoin://) {
    # Mapside join against corresponding partfile(s)
    $result = $original
            . expand_filename_shorthands(
                hadoop_matched_partfiles $f, $ENV{map_input_file});
  } elsif ($f =~ s/^perl://) {
    # Evaluate a Perl expression
    $f =~ s/'/'"'"'/g;
    $result = "${original}perl -e 'print \$_, \"\\n\" for ($f)' |";
  } elsif ($f =~ /(\w*@?[^:]+):(.*)$/) {
    # Access file over SSH
    $result = "${original}ssh -C '$1' cat '$2' |";
  } else {
    return undef;
  }

  $always_make_a_command && $result !~ /\|/ ? "${original}cat '$result' |"
                                            : $result;
}

sub unexpand_filename_shorthands {
  my ($f) = @_;
  $f =~ /^#(.*)/ ? $1 : $f;
}

# Flags
our $verbose    = 0;
our $n_lines    = 0;
our $n_bytes    = 0;
our $start_time = undef;

our $verbose_command = '';
our @verbose_args;
our $verbose_command_formatted = undef;
our $inside_hadoop_job         = length $ENV{mapred_job_id};

our $last_verbose_report = 0;
our $verbose_row         = 0;

# Call it like this:
# while (<>) {
#   be_verbose_as_appropriate length;
#   ...
# }
sub be_verbose_as_appropriate {
  return unless $verbose;
  my ($record_length) = @_;
  $n_lines += !!$record_length;
  $n_bytes += $record_length;
  my $now = time;
  return unless $record_length == 0
             || ($now - $last_verbose_report) * 1000 > VERBOSE_INTERVAL;

  $last_verbose_report = $now;
  $verbose_command_formatted //= join ' ', $verbose_command, @verbose_args;
  $start_time //= $now;
  my $runtime = $now - $start_time || 1;

  unless ($inside_hadoop_job) {
    # Print status updates straight to the terminal
    printf STDERR "\033[%d;1H\033[K%10dl %8.1fl/s %10dk %8.1fkB/s  %s",
                  $verbose_row,
                  $n_lines,
                  $n_lines / $runtime,
                  $n_bytes / 1024,
                  $n_bytes / 1024 / $runtime,
                  $verbose_command_formatted;
  } else {
    # Use Hadoop-specific syntax to update job counters
    $verbose_command_formatted =~ s/[,\n]/_/g;
    printf STDERR "reporter:counter:%s,%s,%d\n", $verbose_command_formatted, @$_
    for (['lines',   $n_lines],
         ['runtime', $runtime],
         ['kB',      $n_bytes / 1024]);

    # Reset variables because Hadoop treats them as incremental
    $n_lines    = 0;
    $n_bytes    = 0;
    $start_time = $now;
  }
}

END {
  be_verbose_as_appropriate 0;
  print STDERR "\n" if $verbose;
}

# This variable will keep track of any state accumulated from --use or --run
# arguments. This is required for --pmap to work correctly.
my @uses;

sub shell_quote {$_[0] =~ /[^-\/\w]/ ? "'" . ($_[0] =~ s/'/'\\''/gr) . "'"
                                     : $_[0]}

sub quote_self {join ' ', $0, map shell_quote($_), @_}

my %explosions = (
  a => '--average',
  A => '--aggregate',
  c => '--count',
  C => '--uncount',
  D => '--drop',
  E => '--every',
  f => '--fields',
  F => '--fieldsplit',
  g => '--group',
  G => '--rgroup',
  i => '--index',
  I => '--indexouter',
  j => '--join',
  J => '--joinouter',
  k => '--keep',
  K => '--remove',
  l => '--log',
  L => '--exp',
  m => '--map',
  M => '--pmap',
  n => '--number',
  o => '--order',
  O => '--rorder',
  p => '--plot',
  P => '--poll',
  q => '--quant',
  s => '--sum',
  S => '--delta',
  T => '--take',
  V => '--variance',
  w => '--with',
);

# Minimum number of required arguments for each function. Numeric arguments are
# automatically forwarded, so are always optional.
my %arity = (
  average    => 0,
  aggregate  => 1,
  count      => 0,
  uncount    => 0,
  delta      => 0,
  drop       => 0,
  every      => 1,
  fields     => 0,
  fieldsplit => 1,
  fold       => 1,
  group      => 0,
  rgroup     => 0,
  index      => 2,
  indexouter => 2,
  join       => 2,
  joinouter  => 2,
  keep       => 1,
  log        => 0,
  exp        => 0,
  map        => 1,
  pmap       => 1,
  number     => 0,
  order      => 0,
  rorder     => 0,
  plot       => 1,
  poll       => 2,
  sum        => 0,
  quant      => 1,
  remove     => 1,
  repeat     => 2,
  sample     => 1,
  take       => 0,
  variance   => 0,
  with       => 1,

  # Commands with no shorthands
  append     => 1,
  prepend    => 1,
  tee        => 1,
  duplicate  => 2,
  partition  => 2,
  splot      => 1,
  sd         => 0,
  mplot      => 1,
  preview    => 0,
  pipe       => 1,
  entropy    => 0,
  hadoop     => 2,
  hadoopcat  => 2,
  hadoopinto => 3,
);

my %usages = (
  average    => 'window size (0 for full average) -- running average',
  aggregate  => 'aggregator fn',
  count      => 'counts by first column value; like uniq -c',
  uncount    => 'the opposite of --count; repeats each row N times',
  delta      => 'value -> difference from last value',
  drop       => 'number of records to drop',
  every      => 'n (returns every nth row)',
  fields     => 'string of digits, each a zero-indexed column selector',
  fieldsplit => 'regexp to use for splitting',
  fold       => 'function that returns true when line should be folded',
  group      => 'sorts ascending, takes optional column list',
  rgroup     => 'sorts descending, takes optional column list',
  index      => 'field index, unsorted pseudofile to join against',
  indexouter => 'field index, unsorted pseudofile to join against',
  join       => 'field index, sorted pseudofile to join against',
  joinouter  => 'field index, sorted pseudofile to join against',
  keep       => 'row filter fn',
  log        => 'optional base (default e)',
  exp        => 'optional base (default e)',
  map        => 'row map fn',
  pmap       => 'row map fn (executed multiple times in parallel)',
  number     => 'prepends line number to each line',
  order      => 'sorts ascending by general numeric value',
  rorder     => 'sorts descending by general numeric value',
  plot       => 'gnuplot arguments',
  poll       => 'interval in seconds, command whose output to collect',
  sum        => 'value -> total += value',
  quant      => 'number to round to',
  remove     => 'inverted row filter fn',
  repeat     => 'repeat count, pseudofile to repeat',
  sample     => 'row selection probability in [0, 1]',
  take       => 'n to take first n, +n to take last n',
  variance   => 'running variance',
  with       => 'pseudofile to join column-wise onto input',

  append     => 'pseudofile; appends its contents to current stream',
  prepend    => 'pseudofile; prepends its contents to current stream',
  tee        => 'shell command; duplicates data to stdin of command',
  duplicate  => 'two shell commands as separate arguments',
  partition  => 'partition id fn, shell command (using {})',
  splot      => 'gnuplot arguments',
  sd         => 'running standard deviation',
  mplot      => 'gnuplot arguments per column, separated by ;',
  preview    => '',
  pipe       => 'shell command to pipe through',
  entropy    => 'running entropy of relative probabilities/frequencies',
  hadoop     => 'hadoop streaming: mapper, reducer (emits outfiles)',
  hadoopcat  => 'hadoop streaming: mapper, reducer (emits data)',
  hadoopinto => 'hadoop streaming into HDFS: mapper, reducer, outpath',
);

my %env_docs = (
  NFU_SORT_BUFFER      => 'default 256M; size of in-memory sort for -g and -o',
  NFU_SORT_PARALLEL    => 'default 4; number of concurrent sorts to run',
  NFU_SORT_COMPRESS    => 'default none; compression program for sort tempfiles',
  NFU_NO_PAGER         => 'if set, nfu will not use "less" to preview stdout',
  NFU_PMAP_PARALLELISM => 'number of subprocesses for -M',
  NFU_MAX_FILEHANDLES  => 'default 64; maximum #subprocesses for --partition',
  NFU_HADOOP_STREAMING => 'absolute location of hadoop-streaming.jar',
  NFU_HADOOP_OPTIONS   => '-D options for hadoop streaming jobs',
  NFU_HADOOP_COMMAND   => 'hadoop executable; e.g. hadoop jar, hadoop fs -ls',
  NFU_HADOOP_TMPDIR    => 'default /tmp; temp dir for hadoop uploads',
);

my %gnuplot_aliases = (
  '%l' => ' with lines',
  '%d' => ' with dots',
  '%i' => ' with impulses',
  '%u' => ' using ',
  '%t' => ' title ',
);

sub expand_gnuplot_options {
  my @transformed_opts;
  for my $opt (@_) {
    $opt =~ s/$_/$gnuplot_aliases{$_}/g for keys %gnuplot_aliases;
    push @transformed_opts, $opt;
  }
  @transformed_opts;
}

sub expand_eval_shorthands {
  my $code = $_[0] =~ s/%(\d+)/\$_[$1]/gr;
  1 while $code =~ s/([a-zA-Z0-9_\)\}\]?\$])
                     \.
                     ([\$_a-zA-Z](?:-[0-9\w?\$]|[0-9_\w?\$])*)
                    /$1\->{'$2'}/x;
  $code;
}

sub parse_join_options {
  my ($f1, $f2, $file) = @_ == 3 ? @_ : ($_[0], 0, $_[1]);
  ($f1 + 1, $f2 + 1, $file);
}

sub compile_eval_into_function {
  my ($code, $name) = @_;
  $code = expand_eval_shorthands $code;
  eval "sub {\n$code\n}"
    or die "failed to compile $name function: $@\n  (code was $code)";
}

sub stateless_unary_fn {
  my ($name, $f) = @_;
  my $arity = $arity{$name};
  ($name, sub {
    my @columns = split //, (@_ > $arity ? shift : undef) // '0';
    while (<>) {
      be_verbose_as_appropriate length;
      chomp;
      my @fs = split /\t/;
      $fs[$_] = $f->($fs[$_], @_) for @columns;
      print row(@fs), "\n";
    }
  });
}

sub stateful_unary_fn {
  my ($name, $setup, $f) = @_;
  my $arity = $arity{$name};
  ($name, sub {
    my @columns = split //, (@_ > $arity ? shift : undef) // '0';
    my %states;
    $states{$_} = $setup->(@_) for @columns;
    while (<>) {
      be_verbose_as_appropriate length;
      chomp;
      my @fs = split /\t/;
      $fs[$_] = $f->($fs[$_], $states{$_}, @_) for @columns;
      print row(@fs), "\n";
    }
  });
}

sub exec_with_stdin {
  open my $fh, '|' . join ' ', map shell_quote($_), @_
    or die "failed to exec @_";
  be_verbose_as_appropriate(length), print $fh $_ while <>;
  close $fh;
}

sub exec_with_diamond {
  if ($verbose || grep /\|/, @ARGV) {
    # Arguments are specified in filenames and involve processes, so use perl
    # to forward data.
    exec_with_stdin @_;
  } else {
    # Faster option: just exec the program in-place. This avoids a layer of
    # interprocess piping. Assume filenames follow arguments.
    exec @_, @ARGV or die "failed to exec @_ @ARGV";
  }
}

sub sort_options {
  my ($column_spec) = @_;
  my @columns       = split //, $column_spec // '';
  return '-S', $ENV{NFU_SORT_BUFFER} || '256M',
         '--parallel=' . ($ENV{NFU_SORT_PARALLEL} || 4),
         (@columns
           ? ('-t', "\t",
              map {('-k', sprintf "%d,%d", $_ + 1, $_ + 1)} @columns)
           : ()),
         ($ENV{NFU_SORT_COMPRESS}
           ? ("--compress-program=$ENV{NFU_SORT_COMPRESS}")
           : ());
}

sub sort_cmd {join ' ', 'sort', sort_options, @_}

sub fifo_for {
  my ($file, @transforms) = @_;
  my $fifo_name = tmpnam;

  mkfifo $fifo_name, 0700 or die "failed to create fifo: $!";

  return $fifo_name if fork;

  my $command = expand_filename_shorthands($file, 1)
              . join '', map {"$_ |"} @transforms;
  open my $into_fifo, '>', $fifo_name
    or die "failed to open fifo $fifo_name for writing: $!";
  open my $from_file, $command
    or die "failed to open file/command $command for reading: $!";

  be_verbose_as_appropriate(length), $into_fifo->print($_) while <$from_file>;
  close $into_fifo;
  close $from_file;

  unlink $fifo_name or warn "failed to unlink temporary fifo $fifo_name: $!";
  exit 0;
}

sub hadoop {
  # Generates a hadoop command string, quoting all args as necessary
  join ' ', $ENV{NFU_HADOOP_COMMAND} // "hadoop", map shell_quote($_), @_;
}

sub hadoop_ls {
  # Now get the output file listing. This is a huge mess because Hadoop is a
  # huge mess.
  # http://stackoverflow.com/questions/21569172/how-to-list-only-file-name-in-hdfs
  my $ls_command = hadoop('fs', '-ls', $_[0])
             . " | sed 1d"
             . " | perl -wlne 'print \"hdfs:\" . +(split \" \",\$_,8)[7]'";
  grep /\/[^_][^\/]*$/, split /\n/, join '', qx/$ls_command/;
}

sub gcd {
  my ($a, $b) = @_;
  while ($b) {
    if ($a > $b) { ($a, $b) = ($b, $a) }
    else         { $b = $a % $b }
  }
  $a;
}

sub hadoop_partfile_n { $1 if $_[0] =~ /[^0-9]([0-9]+)$/ }
sub hadoop_matched_partfile {
  my ($path, $partfile) = @_;
  my @left_files    = hadoop_ls $ENV{mapred_input_dir};
  my @possibilities = hadoop_ls $path;

  my $n = hadoop_partfile_n $partfile;

  # In most cases we'll have the same number of partfiles on each side.
  if (@left_files == @possibilities) {
    my @matched = grep $n == hadoop_partfile_n $_, @possibilities;
    "hdfs:" . join(',', @matched);
  } else {
    die "hadoop_matched_partfile cannot yet handle joins across different "
      . "numbers of shards (this will be fixed at some point); left side has "
      . scalar(@left_files) . " and right has " . scalar(@possibilities);
  }
}

sub hadoop_tempfile {
  # 128 bits of random data
  my $randomness = join '', map sprintf('%04x', rand 65536), 0 .. 8;
  my $dir        = $ENV{NFU_HADOOP_TMPDIR} // '/tmp';
  "$dir/nfu-hadoop-$ENV{USER}-$randomness";
}

sub find_hadoop_streaming {
  my @files = split /\n/, join '', qx|locate "*hadoop-streaming.jar*"|;
  die "failed to locate hadoop streaming jar automatically; "
    . "you should set NFU_HADOOP_STREAMING" unless @files;
  print STDERR "nfu found hadoop streaming jar: $files[0]\n";
  $files[0];
}

sub hadoop_into {
  die "hadoop_into got (@_) but expected (mapper, reducer, outfile)"
    unless @_ == 3;

  my ($mapper, $reducer, $outfile) = @_;
  my $streaming_jar = $ENV{NFU_HADOOP_STREAMING} // find_hadoop_streaming;

  my @input_files;
  my @delete_afterwards;

  # Figure out where our input seems to be coming from. This is a little hacky,
  # but I think it's worthwhile for the flexibility we get.
  #
  # Hadoop jobs output their list of output partfile names because the data is
  # often too large. It's possible we'll get one of these as stdin, so each
  # line will look like "hdfs:/...". In that case, we want to use each as an
  # input file. Otherwise we'll want to upload stdin and all non-HDFS files to
  # HDFS and use those.
  #
  # It's actually a bit tricky to figure out which files started out as hdfs:
  # locations because by now they've all been filename alias-expanded. We need
  # to reverse-engineer the alias if we want to reuse stuff already on HDFS.

  my @other_argv;
  while (@ARGV) {
    local $_ = unexpand_filename_shorthands shift @ARGV;
    if (s/^hdfs://) {
      push @input_files, '-input', $_;
    } else {
      push @other_argv, $_;
    }
  }

  @ARGV = @other_argv;
  my $line = undef;
  unless (-t STDIN) {
    chomp($line), push @input_files, '-input', $line
    while ($line = <STDIN>) =~ s/^hdfs://;
  }

  # At this point $line is either undefined or contains a line we shouldn't
  # have read from stdin (i.e. it's data). If the latter, prepend it to the
  # upload to HDFS.
  if (defined $line) {
    my $tempfile = hadoop_tempfile;
    open my $fh, "| " . hadoop('fs', '-put', '-', $tempfile) . ' 1>&2'
      or die "failed to open hadoop fs -put process for uploading: $!";
    print $fh $line;
    be_verbose_as_appropriate(length), print $fh $_ while <STDIN>;
    close $fh;

    push @input_files,       '-input', $tempfile;
    push @delete_afterwards, $tempfile;
  }

  if (@other_argv) {
    my $tempfile = hadoop_tempfile;
    open my $fh, "| " . hadoop('fs', '-put', '-', $tempfile) . ' 1>&2'
      or die "failed to open hadoop fs -put process for uploading: $!";
    be_verbose_as_appropriate(length), print $fh $_ while <>;
    close $fh;

    push @input_files,       '-input', $tempfile;
    push @delete_afterwards, $tempfile;
  }

  # Now all the input files are in place, so we can kick off the job.
  my $extra_args = $ENV{NFU_HADOOP_OPTIONS} // '';
  my @self_dep   = ('-file', $0);
  my $dirname    = $0 =~ s/\/nfu$//r;

  # We're uploading ourselves, so we'll need a current-directory reference to
  # nfu. When nfu quotes itself, it produces an absolute path instead; this
  # code rewrites those into ./nfu.
  my $transformed_mapper  = $mapper  =~ s|\Q$dirname\E|./|gr;
  my $transformed_reducer = $reducer =~ s|\Q$dirname\E|./|gr;

  my $hadoop_command =
    hadoop('jar', $streaming_jar,
           @self_dep,
           @input_files,
           '-output',  $outfile,
           '-mapper',  $transformed_mapper,
           '-reducer', $transformed_reducer) . $extra_args;

  system $hadoop_command . ' 1>&2'
    and die "failed to execute hadoop command $hadoop_command: $!";

  system hadoop('fs', '-rm', '-r', @delete_afterwards) . ' 1>&2';
  hadoop_ls $outfile;
}

my %functions = (
  group  => sub {exec_with_diamond 'sort', sort_options @_},
  rgroup => sub {exec_with_diamond 'sort', '-r', sort_options @_},
  order  => sub {exec_with_diamond 'sort', '-g', sort_options @_},
  rorder => sub {exec_with_diamond 'sort', '-rg', sort_options @_},

  count => sub {
    # Same behavior as uniq -c, but delimits counts with \t; also takes an
    # optional series of columns to uniq by, rather than using the whole row.
    my @columns = split //, shift // '';
    my $last;
    my @last;
    my $count = -1;

    while (<>) {
      be_verbose_as_appropriate length;
      chomp;

      my @xs = split /\t/;
      @xs   = @xs[@columns] if @columns;
      $last = $_, @last = @xs unless ++$count;

      for (my $i = 0; $i < max scalar(@xs), scalar(@last); ++$i) {
        if (!defined $xs[$i] || !defined $last[$i] || $xs[$i] ne $last[$i]) {
          print "$count\t$last\n";
          $count = 0;
          @last  = @xs;
          $last  = $_;
          last;
        }
      }
    }

    ++$count;
    print "$count\t$last\n";
  },

  uncount => sub {
    while (<>) {
      be_verbose_as_appropriate length;
      my ($n, $line) = split /\t/, $_, 2;
      $line //= "\n";
      print $line for 1..$n;
    }
  },

  index => sub {
    # Inner join by appending joined fields to the end.
    my ($f1, $f2, $join_file) = parse_join_options @_;

    my $sorted_index = fifo_for $join_file, sort_cmd "-t '\t' -k${f2}b,$f2";
    my $command = sort_cmd "-t '\t' -k ${f1}b,$f1" .
                  "| join -t '\t' -1 $f1 -2 $f2 - '$sorted_index'";

    open my $to_join, "| $command" or die "failed to exec $command: $!";
    be_verbose_as_appropriate(length), print $to_join $_ while <>;
    close $to_join;
  },

  indexouter => sub {
    # Outer left join by appending joined fields to the end.
    my ($f1, $f2, $join_file) = parse_join_options @_;

    my $sorted_index = fifo_for $join_file, sort_cmd "-t '\t' -k ${f2}b,$f2";
    my $command = sort_cmd "-t '\t' -k ${f1}b,$f1" .
                  "| join -a 1 -t '\t' -1 $f1 -2 $f2 - '$sorted_index'";

    open my $to_join, "| $command" or die "failed to exec $command: $!";
    be_verbose_as_appropriate(length), print $to_join $_ while <>;
    close $to_join;
  },

  join => sub {
    # Inner join against sorted data by appending joined fields to the end.
    my ($f1, $f2, $join_file) = parse_join_options @_;

    my $sorted_index = fifo_for $join_file;
    my $command = sort_cmd "-t '\t' -k ${f1}b,$f1" .
                  "| join -t '\t' -1 $f1 -2 $f2 - '$sorted_index'";

    open my $to_join, "| $command" or die "failed to exec $command: $!";
    be_verbose_as_appropriate(length), print $to_join $_ while <>;
    close $to_join;
  },

  joinouter => sub {
    # Outer left join against sorted data by appending joined fields to the
    # end.
    my ($f1, $f2, $join_file) = parse_join_options @_;

    my $sorted_index = fifo_for $join_file;
    my $command = sort_cmd "-t '\t' -k ${f1}b,$f1" .
                  "| join -a 1 -t '\t' -1 $f1 -2 $f2 - '$sorted_index'";

    open my $to_join, "| $command" or die "failed to exec $command: $!";
    be_verbose_as_appropriate(length), print $to_join $_ while <>;
    close $to_join;
  },

  with => sub {
    # Like 'paste'. Joins lines with \t.
    my ($f) = @_;
    open my $fh, expand_filename_shorthands $f, 1
      or die "failed to open --with pseudofile $f: $!";
    my ($part1, $part2);
    while (defined($part1 = <>) and defined($part2 = <$fh>)) {
      be_verbose_as_appropriate length($part1) + length($part2);
      chomp $part1;
      chomp $part2;
      print $part1, "\t", $part2, "\n";
    }
    close $fh;
  },

  repeat => sub {
    my ($n, $f) = @_;
    my $count = 0;
    while (!$n || $count++ < $n) {
      open my $fh, expand_filename_shorthands $f, 1
        or die "failed to open --repeat pseudofile $f: $!";
      be_verbose_as_appropriate(length), print while <$fh>;
      close $fh;
    }
  },

  stateful_unary_fn('average',
    sub {my ($size, $n, $total) = ($_[0] // 0, 0, 0);
         [$size, $n, $total, []]},
    sub {
      my ($x, $state) = @_;
      my ($size, $n, $total, $window) = @$state;
      $total += $x;
      ++$n;
      my $v = $total / ($n > $size && $size ? $size : $n);
      $total -= shift @$window if $size and push(@$window, $x) >= $size;
      $$state[1] = $n;
      $$state[2] = $total;
      $v;
    }),

  aggregate => sub {
    my $f = compile_eval_into_function $_[0], 'aggregate function';
    my @columns;
    while (my $line = <>) {
      be_verbose_as_appropriate length $line;
      chomp $line;
      my @fields = split /\t/, $line;

      # Two cases here. If the new record is compatible with the most recent
      # existing one, or there aren't any existing ones, then group it and
      # don't call the aggregator yet.
      #
      # If we see a change, then call the aggregator and empty out the group.
      #
      # Note that the aggregator function is called on columns, not rows.

      my $n = @columns && @{$columns[0]};
      if (!$n or $fields[0] eq ${$columns[0]}[0]) {
        $columns[$_][$n] = $fields[$_] for 0 .. $#fields;
      } else {
        $_ = ${$columns[0]}[0];
        print $_, "\n" for $f->(@columns);
        @columns = ();
        $columns[$_][0] = $fields[$_] for 0 .. $#fields;
      }
    }
    if (@columns) {
      $_ = ${$columns[0]}[0];
      print $_, "\n" for $f->(@columns);
    }
  },

  fold => sub {
    my $f = compile_eval_into_function $_[0], 'fold function';
    my @saved;
    while (my $line = <>) {
      be_verbose_as_appropriate length $line;
      chomp $line;
      if ($f->(split /\t/, $line)) {
        push @saved, $line;
      } else {
        print row(@saved), "\n" if @saved;
        @saved = ($line);
      }
    }
    print row(@saved), "\n" if @saved;
  },

  stateless_unary_fn('log', sub {
    my ($x, $base) = @_;
    my $log = log $x;
    $log /= log $base if defined $base;
    $log;
  }),

  stateless_unary_fn('exp', sub {
    my ($x, $base) = @_;
    defined $base ? $base ** $x : exp $x;
  }),

  stateless_unary_fn('quant', sub {
    my ($x, $quantum) = @_;
    $quantum ||= 1.0;
    my $sign = $x < 0 ? -1 : 1;
    int(abs($x) / $quantum + 0.5) * $quantum * $sign;
  }),

  # Note: this needs to be stdin; otherwise "nfu -p %l filename" will fail
  # (since exec_with_diamond trieds to pass filename straight into gnuplot).
  plot => sub {
    exec_with_stdin 'gnuplot',
                    '-e',
                    'plot "-" ' . join(' ', expand_gnuplot_options @_),
                    '-persist';
  },

  splot => sub {
    exec_with_stdin 'gnuplot',
                    '-e',
                    'splot "-" ' . join(' ', expand_gnuplot_options @_),
                    '-persist';
  },

  mplot => sub {
    my @gnuplot_options = split /;/, join ' ', expand_gnuplot_options @_;
    my $fname = tmpnam;
    open my $fh, '>', $fname or die "failed to open tempfile for mplot: $!";
    $fh->print($_) while <>;
    close $fh;
    system 'gnuplot', '-e',
           'plot ' . join(',', map("\"$fname\" $_", @gnuplot_options)),
           '-persist';

    # HACK: the problem is that gnuplot internally forks a subprocess for the
    # plot window, which we won't be able to see from here (that I know of). If
    # we delete the file before that subprocess exits, then any zoom operations
    # will cause gnuplot to abruptly exit.
    #
    # I'm sure there's a better way to solve this, but for now this should do
    # the job for now.
    unless (fork) {
      setsid;
      close STDIN;
      close STDOUT;
      unless (fork) {
        sleep 3600;
        unlink $fname or die "failed to unlink $fname: $!";
      }
    }
  },

  poll => sub {
    my ($sleep, $command) = @_;
    die "usage: --poll sleep-amount 'command ...'"
      unless defined $sleep and defined $command;
    system($command), sleep $sleep while 1;
  },

  stateful_unary_fn('delta',
    sub {[0]},
    sub {my ($x, $state) = @_;
         my $v = $x - $$state[0];
         $$state[0] = $x;
         $v}),

  stateful_unary_fn('sum',
    sub {[0]},
    sub {my ($x, $state) = @_;
         $$state[0] += $x}),

  stateful_unary_fn('variance',
    sub {[0, 0, 0]},
    sub {my ($x, $state) = @_;
         $$state[0] += $x;
         $$state[1] += $x * $x;
         $$state[2]++;
         my ($sx, $sx2, $count) = @$state;
         ($sx2 - ($sx * $sx / $count)) / ($count - 1 || 1)}),

  stateful_unary_fn('sd',
    sub {[0, 0, 0]},
    sub {my ($x, $state) = @_;
         $$state[0] += $x;
         $$state[1] += $x * $x;
         $$state[2]++;
         my ($sx, $sx2, $count) = @$state;
         sqrt(($sx2 - ($sx * $sx / $count)) / ($count - 1 || 1))}),

  stateful_unary_fn('entropy',
    # state contains [$total, $entropy_so_far] and uses the following
    # associative combiner (where F(X) = frequency of X, unscaled probability):
    #
    # let t = F(A) + F(B)
    # H(A + B) = F(A)/t * (-log(F(A)/t) + H(A))
    #          + F(B)/t * (-log(F(B)/t) + H(B))

    sub {[0, 0]},
    sub {my ($x, $state) = @_;
         my ($f0, $h0)   = @$state;
         my $f           = $$state[0] += $x;
         my $p           = $x  / $f;
         my $p0          = $f0 / $f;
         $$state[1]      = $p0 * (($p0 > 0 ? -log($p0) / log(2) : 0) + $h0)
                         + $p  *  ($p  > 0 ? -log($p)  / log(2) : 0)}),

  take => sub {
    if ($_[0] =~ s/^\+//) {
      # Take last n, so we need a line queue
      my @q;
      my $i = 0;
      be_verbose_as_appropriate(length), $q[$i++ % $_[0]] = $_ while <>;
      print for @q[$i % $_[0] .. $#q];
      print for @q[0 .. $i % $_[0] - 1];
    } else {
      my $n = $_[0] // 1;
      while (<>) {
        be_verbose_as_appropriate length;
        last if --$n < 0;
        print;
      }
    }
  },

  sample => sub {
    while (<>) {
      be_verbose_as_appropriate length;
      print if rand() < $_[0];
    }
  },

  drop => sub {
    my $n = $_[0] // 1;
    while (<>) {
      be_verbose_as_appropriate length;
      last if --$n <= 0;
    }
    be_verbose_as_appropriate(length), print while <>;
  },

  map => sub {
    my $f = compile_eval_into_function $_[0], 'map function';
    while (my $line = <>) {
      be_verbose_as_appropriate length $line;
      chomp $line;
      my @xs = split /\t/, $line;
      print "$_\n" for $f->(@xs);
    }
  },

  pmap => sub {
    my @fhs;
    my $wbits = '';
    my $wout  = '';
    my $i     = 0;

    for (1 .. $ENV{NFU_PMAP_PARALLELISM} // 16) {
      my $mapper = quote_self @uses, '--map', $_[0];
      open my $fh, "| $mapper"
        or die "failed to open child process $mapper: $!";

      vec($wbits, fileno($fh), 1) = 1;
      push @fhs, $fh;
    }

    while (<>) {
      be_verbose_as_appropriate;
      select undef, $wout = $wbits, undef, undef;
      ++$i until vec($wout, fileno $fhs[$i % @fhs], 1);
      syswrite $fhs[$i++ % @fhs], $_;
    }
    close for @fhs;
  },

  keep => sub {
    my $f = compile_eval_into_function $_[0], 'keep function';
    while (my $line = <>) {
      be_verbose_as_appropriate length $line;
      chomp $line;
      my @xs = split /\t/, $line;
      print row(@xs), "\n" if $f->(@xs);
    }
  },

  remove => sub {
    my $f = compile_eval_into_function $_[0], 'remove function';
    while (my $line = <>) {
      be_verbose_as_appropriate length $line;
      chomp $line;
      my @xs = split /\t/, $line;
      print row(@xs), "\n" unless $f->(@xs);
    }
  },

  every => sub {
    my ($n) = @_;
    my $i = 0;
    while (<>) {
      be_verbose_as_appropriate length;
      print unless $i++ % $n;
    }
  },

  fields => sub {
    my ($fields)   = @_;
    my $everything = $fields =~ s/\.$//;
    my @fs         = split //, $fields;
    $everything &&= 1 + max @fs;

    while (<>) {
      be_verbose_as_appropriate length;
      chomp;
      my @xs = split /\t/;
      my @ys = @xs[@fs];
      push @ys, @xs[$everything .. $#xs] if $everything;
      print join("\t", map $_ // '', @ys), "\n";
    }
  },

  fieldsplit => sub {
    my $delim = qr/$_[0]/;
    while (<>) {
      be_verbose_as_appropriate length;
      chomp;
      print join("\t", split /$delim/), "\n";
    }
  },

  number => sub {
    my $n = 0;
    while (<>) {
      be_verbose_as_appropriate length;
      chomp;
      print row(++$n, $_), "\n";
    }
  },

  prepend => sub {
    open my $fh, expand_filename_shorthands $_[0], 1
      or die "failed to open --prepend pseudofile $_[0]: $!";
    be_verbose_as_appropriate(length), print while <$fh>;
    close $fh;
    print while <>;
  },

  append => sub {
    open my $fh, expand_filename_shorthands $_[0], 1
      or die "failed to open --append pseudofile $_[0]: $!";
    print while <>;
    be_verbose_as_appropriate(length), print while <$fh>;
    close $fh;
  },

  pipe => sub {
    open my $fh, "| $_[0]" or die "failed to launch $_[0]: $!";
    while (<>) {
      be_verbose_as_appropriate length;
      $fh->print($_);
    }
    close $fh;
  },

  tee => sub {
    open my $fh, "| $_[0]" or die "failed to launch $_[0]: $!";
    while (<>) {
      be_verbose_as_appropriate length;
      $fh->print($_);
      print;
    }
    close $fh;
  },

  duplicate => sub {
    open my $fh1, "| $_[0]" or die "failed to launch $_[0]: $!";
    open my $fh2, "| $_[1]" or die "failed to launch $_[1]: $!";
    while (<>) {
      be_verbose_as_appropriate length;
      $fh1->print($_);
      $fh2->print($_);
    }
    close $fh1;
    close $fh2;
  },

  partition => sub {
    my ($splitter, $cmd) = @_;
    my %fhs;
    my $f = compile_eval_into_function $splitter, 'partition function';

    my @open_partitions;
    while (<>) {
      be_verbose_as_appropriate length;
      my $line = $_;
      my $p    = $f->(split /\t/, $line);
      unless (exists $fhs{$p}) {
        my $cmdsub = $cmd =~ s/\{\}/$p/gr;
        open $fhs{$p}, "| $cmdsub" or die "failed to launch $cmdsub: $!";
        push @open_partitions, $p;
      }
      $fhs{$p}->print($line);
      close($fhs{$p = shift @open_partitions}), delete $fhs{$p}
        while @open_partitions > ($ENV{NFU_MAX_FILEHANDLES} // 64);
    }
    close for values %fhs;
  },

  hadoop => sub {
    print "$_\n" for hadoop_into @_, hadoop_tempfile;
  },

  hadoopcat => sub {
    my $filename = hadoop_tempfile;
    system hadoop('fs', '-text', map s/^hdfs://r, hadoop_into @_, $filename);
    system hadoop('fs', '-rm', '-r', $filename) . ' 1>&2';
  },

  hadoopinto => sub {printf "hdfs:$_\n" for hadoop_into @_},

  preview => sub {
    my $have_less = !system 'which less > /dev/null';
    my $have_more = !system 'which more > /dev/null';

    my $less_program = $have_less ? 'less'
                     : $have_more ? 'more' : 'cat';

    exec_with_diamond $less_program;
  },
);

# Print usage if the user clearly doesn't know what they're doing.
if (@ARGV ? $ARGV[0] =~ /^-[h?]$/ || $ARGV[0] =~ /^--(usage|help)$/
          : -t STDIN) {

  # Some checks for me to make sure I'm keeping the code well-maintained
  exists $functions{$_}         or die "no function for $_" for keys %usages;
  exists $usages{$_}            or die "no usage for $_"    for keys %functions;
  exists $arity{$_}             or die "no arity for $_"    for keys %usages;
  exists $usages{$_ =~ s/--//r} or die "no usage for $_"
    for values %explosions, keys %usages;

  print STDERR "usage: nfu [prefix-commands...] [input-files...] commands...\n";
  print STDERR "where prefix commands are:\n\n";

  print STDERR "  documentation (not used with normal commands):\n";
  print STDERR "     --explain           <other-options>\n";
  print STDERR "     --expand-pseudofile <filename>\n";
  print STDERR "     --expand-code       <code>\n";
  print STDERR "     --expand-gnuplot    <gnuplot options>\n";

  print STDERR "\n  pipeline modifiers:\n";
  print STDERR "     -Q|--quote  -- quotes args: eval \$(nfu --quote ...)\n";
  print STDERR "     --use       <file.pl>\n";
  print STDERR "     --run       <perl code>\n";

  print STDERR "\nand each command is one of the following:\n\n";

  my $len = 1 + max map length, keys %usages;
  my %short_lookup;
  $short_lookup{$explosions{$_} =~ s/^--//r} = $_ for keys %explosions;

  for my $cmd (sort keys %usages) {
    my $short = $short_lookup{$cmd};
    $short = defined $short ? "-$short|" : '   ';
    printf STDERR "  %s--%-${len}s(%d) %s\n",
                  $short,
                  $cmd,
                  $arity{$cmd},
                  $usages{$cmd} ? $arity{$cmd} ? "<$usages{$cmd}>"
                                               : "-- $usages{$cmd}" : '';
  }

  print STDERR "\ngnuplot expansions:\n\n";
  printf STDERR "  %2s -> '%s'\n", $_, $gnuplot_aliases{$_}
  for sort keys %gnuplot_aliases;

  my $env_len = 1 + max map length, keys %env_docs;
  print STDERR "\nenvironment variables:\n\n";
  printf STDERR "  %-${env_len}s %s\n", $_, $env_docs{$_}
  for sort keys %env_docs;

  print STDERR "\n";
  print STDERR "see https://github.com/spencertipping/nfu for documentation\n";
  print STDERR "\n";

  exit 1;
}

if (@ARGV and $ARGV[0] eq '--quote' || $ARGV[0] =~ s/^-Q/-/) {
  # Quote all other arguments so a shell will parse them correctly.
  shift @ARGV if $ARGV[0] eq '--quote' || $ARGV[0] eq '-';
  print quote_self(@ARGV), "\n";
  exit 0;
}

if (@ARGV && $ARGV[0] =~ /^--expand/) {
  my ($command, $x, @others) = @ARGV;
  if ($command =~ /-pseudofile$/) {
    print expand_filename_shorthands($x) // '<invalid/nonexistent>', "\n";
  } elsif ($command =~ /-code$/) {
    print expand_eval_shorthands($x), "\n";
  } elsif ($command =~ /-gnuplot$/) {
    print expand_gnuplot_options($x), "\n";
  } else {
    print STDERR "unknown expansion command: $command\n";
    exit 1;
  }
  exit 0;
}

my $reader        = undef;
my @parsed        = ();
my $explain       = 0;
my @args_to_parse = @ARGV;

@ARGV = ();

sub explode {
  return $_[0] unless $_[0] =~ s/^-([^-])/$1/;
  map {$explosions{$_} // $_} grep length, split /([-+.\d]*),?/, $_[0];
}

# First parse through all of the options, pull out stray files, and replace
# @ARGV. This enables <> in the worker subs. (The "right way" to do this would
# be to chain the commands' inputs; then I could remove the branch in the
# for-loop below. But I'm too lazy.)
while (@args_to_parse) {
  unshift @args_to_parse, explode shift @args_to_parse;
  (my $command = shift @args_to_parse) =~ s/^--//;

  if (defined(my $arity = $arity{$command})) {
    my @args;
    push @args, shift @args_to_parse
    while @args_to_parse && (--$arity >= 0
                             || ! -e $args_to_parse[0]
                                && $args_to_parse[0] =~ /^[-+]?\d+/);
    push @parsed, [$command, @args];
  } elsif ($command eq 'run') {
    my $x = shift @args_to_parse;
    push @uses, 'run', $x;
    eval $x;
    die "failed to run '$x': $@" if $@;
  } elsif ($command eq 'use') {
    my $x = shift @args_to_parse;
    push @uses, 'use', $x;
    eval read_file $x;          # instead of 'do' so we get shorthands
    die "failed to use '$x': $@" if $@;
  } elsif ($command eq 'explain') {
    $explain = 1;
  } elsif ($command eq 'verbose' || $command eq 'v') {
    print STDERR "\033[2J";
    $verbose = 1;
  } else {
    my $f = expand_filename_shorthands $command;
    die "nonexistent pseudofile: $command" unless defined $f;
    push @ARGV, $f;
  }
}

# Open output in an interactive previewer if...
push @parsed, ['preview'] if !$ENV{NFU_NO_PAGER}    # we can page
                          && (!-t STDIN || @ARGV)   # not interacting for input
                          && -t STDOUT;             # interacting for output

if ($explain) {
  # Explain what we would have done with the given command line.
  printf "file\t%s\n", $_ for @ARGV;
  printf "--%s\t%s\n", ${$_}[0], join "\t",
                                 map "'$_'", @{$_}[1 .. $#$_] for @parsed;
} elsif (@parsed) {
  # Note: the loop below uses pipe/fork/dup2 instead of a more idiomatic Open2
  # call. I don't have a good reason for this other than to figure out how the
  # low-level stuff worked.
  for (my $i = 0; $i < @parsed; ++$i) {
    my ($command, @args) = @{$parsed[$i]};

    # Here's where things get fun. The question right now is, "do we need to
    # fork, or can we run in-process?" -- i.e. are we in the middle, or at the
    # end? When we're in the middle, we want to redirect STDOUT to the pipe's
    # writer and fork; otherwise we run in-process and write directly to the
    # existing STDOUT.
    ++$verbose_row;
    if ($i < @parsed - 1) {
      # We're in the middle, so allocate a pipe and fork.
      pipe my($new_reader), my($writer);
      $verbose_command = $command;
      @verbose_args    = @args;
      unless (fork) {
        # We're the child, so do STDOUT redirection.
        close $new_reader or die "failed to close pipe reader: $!";
        dup2(fileno($reader), 0) or die "failed to dup input: $!"
          if defined $reader;
        dup2(fileno($writer), 1) or die "failed to dup stdout: $!";

        close $reader or die "failed to close reader: $!" if defined $reader;
        close $writer or die "failed to close writer: $!";

        # The function here may never return.
        $functions{$command}->(@args);
        exit;
      } else {
        close $writer or die "failed to close pipe writer: $!";
        $reader = $new_reader;
      }
    } else {
      # We've hit the end of the chain. Preserve stdout, redirect stdin from
      # current reader.
      dup2(fileno($reader), 0) or die "failed to dup input: $!"
        if defined $reader;
      $verbose_command = $command;
      @verbose_args    = @args;
      $functions{$command}->(@args);
    }

    # Prevent <> from reading files after the first iteration (this is such a
    # hack).
    @ARGV = ();
  }
} else {
  # Behave like cat, which is useful for auto-decompressing things.
  be_verbose_as_appropriate(length), print while <>;
}
